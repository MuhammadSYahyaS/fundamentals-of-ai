{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visual Studio Code is highly recommended to open this notebook. I used KaTeX equations in Markdown for writing equations and it might not rendered very well using other than Visual Studio Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyTorch implementation of a simple feedforward network as seen at Figure 21.3 in `Russell S. J. & Norvig P. (2020). Artificial intelligence : a modern approach (4th ed.). Pearson.` book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 21.3](images/fig_21_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write an expression for the output of that network as follows (taken from Equation 21.2 of the book)\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\^{y} &= g_5(in_5) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}a_3 + w_{4,5}a_4) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}g_3(in_3) + w_{4,5}g_4(in_4)) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}g_3(w_{0,3} + w_{1,3}x_1 + w_{2,3}x_2)\n",
    "+ w_{4,5}g_4(w_{0,4} + w_{1,4}x_1 + w_{2,4}x_2))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the activation functions of $g_3$ and $g_4$ are using a ReLU function, and $g_5$ is just a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        v = F.relu(self.fc1(x))\n",
    "        z = self.fc2(v)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleFeedForward(\n",
       "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SimpleFeedForward()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we kept the inital weights random.\n",
    "But for the sake of simplicity of our study, lets initialize the weights with easy numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True) \n",
      "\n",
      "fc1.bias Parameter containing:\n",
      "tensor([4., 5.], requires_grad=True) \n",
      "\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[6., 7.]], requires_grad=True) \n",
      "\n",
      "fc2.bias Parameter containing:\n",
      "tensor([8.], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    last_v = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        p_numel = param.numel()\n",
    "        param.data = nn.parameter.Parameter(\n",
    "            torch.arange(last_v, last_v+p_numel, dtype=torch.float32).reshape(param.shape))\n",
    "        print(name, param, \"\\n\")\n",
    "        last_v += p_numel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let a training example below is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[-2, 1]])\n",
    "y = torch.Tensor([[64]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for our training example. This process is also usually called as forward-pass/forward-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[66.]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_hat = net(x)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is $66$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually calculate the output of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 21.3b](images/fig_21_3b.png)\n",
    "\n",
    "![Forward-pass of Figure 21.3b](images/fig_21_3b_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is same at $66$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the gradient for the\n",
    "network with respect to our previous single training example $(\\mathbf{x},y)$. (For multiple\n",
    "examples, the gradient is just the sum of the gradients for the individual examples.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the squared loss function $L_2$ is used.\n",
    "\n",
    "$$ L_2 = (y-\\^{y})^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the gradient of the loss with respect to (w.r.t.) the weights using the chain rule \n",
    "$$ {dy \\over dx} = {dy \\over du}{du \\over dx}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the gradient of our $L_2$ loss w.r.t. $w_{3,5}$ should be\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = {\\partial L_2 \\over \\partial \\^{y}}{\\partial \\^{y} \\over \\partial in_5}{\\partial in_5 \\over \\partial w_{3,5}}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial \\^{y}} = {\\partial \\over \\partial \\^{y}}(y-\\^{y})^2 = 2(y − \\^{y})(-1) = −2(y − \\^{y}),\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "{\\partial \\^{y} \\over \\partial in_5} = {\\partial \\over \\partial in_5}(g_5(in_5)) = g_{5}'(in_5).\n",
    "$$\n",
    "\n",
    "Since $w_{0,5}$ and $w_{4,5}a_4$ do not depend on $w_{3,5}$, also $a_3$ does not depend on $w_{3,5}$,\n",
    "$$ {\\partial in_5 \\over \\partial w_{3,5}} = {\\partial \\over \\partial w_{3,5}}(w_{0,5} + w_{3,5}a_3 + w_{4,5}a_4) = a_3.\n",
    "$$\n",
    "\n",
    "Finally, we have \n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = −2(y − \\^{y}) g_{5}'(in_5) a_3.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to compute the gradient of our $L_2$ loss w.r.t. $w_{3,5}$ using that equation for our previous training example. \n",
    "\n",
    "Since $g_5$ is just a linear function $g_5(in_5)=in_5$, then $g_{5}'(in_5)=1$, so\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = −2(64 − 66) \\cdot 1 \\cdot 5 = 20.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can update our $w_{3,5}$ (with learning rate $\\alpha=1.0$)\n",
    "$$\n",
    "w_{3,5} \\colonequals w_{3,5} - \\alpha {\\partial L_2 \\over \\partial w_{3,5}} = 6 - 1 \\cdot 20 = -14.\n",
    "$$\n",
    "The updated weight of $w_{3,5}$ is $-14$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a slighty more difficult case, the gradient of our $L_2$ loss w.r.t. $w_{1,3}$,\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} = {\\partial L_2 \\over \\partial \\^{y}}{\\partial \\^{y} \\over \\partial in_5}{\\partial in_5 \\over \\partial in_3}{\\partial in_3 \\over \\partial w_{1,3}}.\n",
    "$$\n",
    "\n",
    "As we can see, the first few steps are identical, so we can use our previous derived functions, so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} &= −2(y − \\^{y}) g_{5}'(in_5){\\partial \\over \\partial in_3}(w_{3,5}a_3){\\partial in_3 \\over \\partial w_{1,3}} \\\\\n",
    "\n",
    "&= −2(y − \\^{y}) g_{5}'(in_5)w_{3,5}{\\partial \\over \\partial in_3}g_3(in_3){\\partial \\over \\partial w_{1,3}}(w_{0,3}+w_{1,3}x_1+w_{2,3}x_2) \\\\\n",
    "\n",
    "&= −2(y − \\^{y}) g_{5}'(in_5)w_{3,5}g_{3}'(in_3)x_1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplification in the last line because $w_{0,3}$ and $w_{2,3}x_2$ do not depend on $w_{1,3}$, also $x_1$ does not depend on any others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to compute the gradient of our $L_2$ loss w.r.t. $w_{1,3}$ using that equation for our previous training example. \n",
    "\n",
    "The $g_3$ is a rectified linear function\n",
    "$$\n",
    "g_3(in_3)=\\begin{cases}\n",
    "   in_3 &\\text{if } in_3 >= 0 \\\\\n",
    "   0 &\\text{if } in_3 < 0\n",
    "\\end{cases}\n",
    "\\\\\\enspace\\\\\n",
    "g_{3}'(in_3)=\\begin{cases}\n",
    "   1 &\\text{if } in_3 >= 0 \\\\\n",
    "   0 &\\text{if } in_3 < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} = −2(64 − 66) \\cdot 1 \\cdot 6 \\cdot 1 \\cdot (-2) = -48.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can update our $w_{1,3}$ (with learning rate $\\alpha=1.0$)\n",
    "$$\n",
    "w_{1,3} \\colonequals w_{1,3} - \\alpha {\\partial L_2 \\over \\partial w_{1,3}} = 0 - 1 \\cdot (-48) = 48.\n",
    "$$\n",
    "The updated weight of $w_{1,3}$ is $48$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was... pretty tedious, huh?\n",
    "\n",
    "No worries! We can compute such gradients by **automatic differentiation** method, which applies the rules of calculus in a systematic way.\n",
    "\n",
    "In our study, we will continue using PyTorch. Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use mean squared error loss function. The \"mean\" term is doesn't matter in our case, since we use only a single training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do backward-pass/back-propagation to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use stochastic gradient descent (SGD) for updating our network parameters (weights). The \"stochastic\" term is doesn't matter in our case, since we use only a single training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we adjust/update the weights of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[ 48., -23.],\n",
      "        [ 58., -25.]], requires_grad=True) \n",
      "\n",
      "fc1.bias Parameter containing:\n",
      "tensor([-20., -23.], requires_grad=True) \n",
      "\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-14.,  -9.]], requires_grad=True) \n",
      "\n",
      "fc2.bias Parameter containing:\n",
      "tensor([4.], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for name, param in net.named_parameters():\n",
    "        print(name, param, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the updated weight are the same with our manual calculation, where $w_{3,5}$ is $-14$ and $w_{1,3}$ is $48$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can freely doing experimentation on different network structures, activation functions, loss functions, and forms of composition without having to do lots of calculus to derive a new learning algorithm for each experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ai_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45806f2a31fc2394908bf2aae38bc8f96498b1e9c39d8308e884e6256764b6c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
